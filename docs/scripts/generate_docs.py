# mypy: ignore-errors


import calendar
import re
import subprocess  # noqa: S404
import sys
from codecs import decode, encode
from functools import cmp_to_key

import pydot
from bs4 import BeautifulSoup
from bs4.builder._htmlparser import HTMLParserTreeBuilder  # noqa: PLC2701
from datauri import DataURI
from pygments import highlight
from pygments.formatters import HtmlFormatter
from pygments.lexers import find_lexer_class_by_name, guess_lexer


class MyTreeBuilder(HTMLParserTreeBuilder):
    DEFAULT_PRESERVE_WHITESPACE_TAGS = {"pre", "textarea", "code"}  # noqa: RUF012


path = "src/cronspell/cronspell"
graph = pydot.graph_from_dot_file(f"{path}.dot")[0]
node = graph.get_node("match_rules")[0]

table = re.sub(re.compile(r"(^< )|( >$)"), "", node.get_label())

tag = BeautifulSoup(table, "html.parser")


with open(f"{path}.tx", encoding="utf-8") as file:
    src_lines = [*file.readlines()]


with open(f"{path}.pu", encoding="utf-8") as file:
    plantuml = re.sub(re.compile(r"\nlegend[\S\s\n]+\nend legend\s*\n", re.MULTILINE), "\n\n", file.read())

with open(f"{path}.pu", encoding="utf-8", mode="+w") as file:
    file.write(plantuml)
    file.close()


def get_compare_func(item):
    return re.compile(rf"^\s*{item.findChild("b").get_text()}\s*:").match


def compare(item1, item2):
    a = next((idx for idx, x in enumerate(src_lines) if get_compare_func(item1)(x)), 1)
    b = next((idx for idx, x in enumerate(src_lines) if get_compare_func(item2)(x)), 1)
    return a - b


rows_sorted = sorted(tag.findChildren("tr"), key=cmp_to_key(compare))


def get_example(regex, short=False):  # noqa: FBT002
    test = re.compile(regex)

    suggested = ""

    with calendar.different_locale(locale=("EN_US", "UTF-8")):
        candidates = [
            *calendar.day_name,
            *calendar.month_name,
            "Year",
            "Day",
            "Month",
            "Hour",
            "Minute",
            "Second",
            "Week",
            "@cw",
            "@m",
            "@y",
            "{",
            "}",
            "now",
            "1979-01-01T00:00:00+00:00",
            "/* inline comment */",
        ]
    if short:
        suggested = next(
            (
                x.replace("@", "%")
                for x in [
                    "// eol comment",
                    "m",
                    *[x[:0] for x in candidates],
                    *[x[:1] for x in candidates],
                    *[x[:3] for x in candidates],
                    *[x[:4] for x in candidates],
                ]
                if test.match(x.replace("@", "%"))
            ),
            "",
        )

    if not short or (suggested == ""):
        suggested = next((x for x in candidates if test.match(x)), "")

    if short and suggested in {"}", "{", "1979-01-01T00:00:00+00:00"}:
        suggested = ""

    if short and test.match(_suggested := suggested.lower()):
        suggested = _suggested

    if short and suggested.startswith("%") and test.match(_suggested := suggested.upper()):
        suggested = _suggested

    if short and suggested == "now":
        suggested = "now[Europe/Berlin]"

    if not short and suggested == "now":
        suggested = ""

    return suggested


with open("README.md", encoding="utf-8") as file:
    # <!-- start autogenerated: legend -->
    # <!-- end autogenerated: legend -->

    # <!-- start autogenerated: diag -->
    # <!-- end autogenerated: diag -->

    def get_cells(node):
        pattern = decode(encode(node.findChildren("td")[1].get_text(), "utf-8", "backslashreplace"), "utf-8")
        return [
            node.findChild("b").get_text(),
            pattern,
            get_example(pattern),
            get_example(pattern, True),
        ]

    rows = [get_cells(x) for x in rows_sorted]

    section_stop_at = {"Now", "MonthModulo", "S", "Sun", "Dec"}

    def get_row_nodes(x):
        # not documenting ISODate
        if next(iter(x), "") == "ISODate":
            return []

        rows = [BeautifulSoup().new_tag("tr")]
        for idx, y in enumerate(x):
            node = BeautifulSoup().new_tag("td")
            val = y.strip()

            if idx == 0:
                em = BeautifulSoup().new_tag("em")
                em.string = val
                node.append(em)
            else:
                language = "cpp" if val.startswith("/*") or val.startswith("//") else "yaml"
                language = "graphviz" if val.startswith("@") or val.startswith("%") or "now[" in val else language
                lexer = guess_lexer(val) if idx == 1 else find_lexer_class_by_name(language)()

                node.append(
                    BeautifulSoup(
                        highlight(val, lexer, HtmlFormatter()), features="html.parser", builder=MyTreeBuilder()
                    )
                )

            rows[0].append(node)

        if next(iter(x), "") in section_stop_at:
            row = BeautifulSoup().new_tag("tr")
            node = BeautifulSoup().new_tag("td")
            node["colspan"] = len(x)
            row.append(node)
            rows.append(row)

        return rows

    rows = [item for it in [get_row_nodes(x) for x in rows] for item in it]

    legend = BeautifulSoup("<table></table>", features="html.parser", builder=MyTreeBuilder()).table

    cpt = BeautifulSoup().new_tag("caption")
    cpt.string = "Legend"
    legend.append(cpt)

    header_row = BeautifulSoup().new_tag("tr")
    for x in ["\n\n $f$ \n\n", "Pattern", "Example", "Alternative Example"]:
        node = BeautifulSoup().new_tag("td")
        node.string = x
        node["scope"] = "col"
        header_row.append(node)

    for row in [header_row, *rows]:
        legend.append(row)

    legend_html = legend.prettify()
    legend_html = re.sub(r"\n\s*\$", r"\n\n$", legend_html)
    legend_html = re.sub(r"\$\s*\n", r"$\n\n", legend_html)

    part_a, part_b = re.split(
        r"\n<!-- start autogenerated: legend -->[\S\s\n]+<!-- end autogenerated: legend -->\n",
        file.read(),
        flags=re.IGNORECASE | re.MULTILINE,
        maxsplit=0,
    )

    doc = "\n".join(
        [part_a, "<!-- start autogenerated: legend -->", legend_html, "<!-- end autogenerated: legend -->", part_b]
    )


with open("README.md", encoding="utf-8", mode="+w") as file:
    file.write(doc)
    file.close()


sys.exit(0)

with open("README.md", encoding="utf-8") as file:
    # <!-- start autogenerated: cronspell -->
    # <!-- end autogenerated: cronspell -->

    mksvg = subprocess.run(  # noqa: S603
        ["plantuml", f"{path}.pu", "-tsvg", "-Slinetype=polyline", "-Smonochrome=true", "-Sshadowing=false"],  # noqa: S607
        check=False,
    )

    with open(f"{path}.svg", encoding="utf-8") as svgsrc:
        raw = svgsrc.read()

    with open(f"{path}.svg", encoding="utf-8", mode="+w") as svgsrc:
        svgsrc.write(raw.replace("#FFFFFF", "#FFFFFF99").replace("#383838", "#0e3f3ae3").replace("#000000", "#0e3f3a"))

    svguri = str(DataURI.from_file(f"{path}.svg"))

    part_a, part_b = re.split(
        r"\n<!-- start autogenerated: cronspell -->[\S\s\n]+<!-- end autogenerated: cronspell -->\n",
        file.read(),
        flags=re.IGNORECASE | re.MULTILINE,
        maxsplit=0,
    )

    doc = "\n".join(
        [
            part_a,
            "<!-- start autogenerated: cronspell -->",
            "\n",
            f'<img src="{svguri}" />',
            "\n",
            "<!-- end autogenerated: cronspell -->",
            part_b,
        ]
    )

with open("README.md", encoding="utf-8", mode="+w") as file:
    file.write(doc)
    file.close()
